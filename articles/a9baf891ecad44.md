---
title: "Whisperでボイスコマンドを作ってみた"
emoji: "🐕"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AI"]
published: true
publication_name: ambr_inc
---

:::message
本記事は2023/09/20に投稿した記事を移植したものです。
:::

### はじめに

初めまして！ambrの VRxAI Laboratory でAIの開発に取り組んでいる木村と申します。

[VRxAI Laboratory](https://prtimes.jp/main/html/rd/p/000000026.000043299.html) は昨今進化の目覚ましいAIの技術を取り入れて、プロダクトの体験価値を一層向上させることを目的として新設されたチームです。

今回はVRxAI Laboratory プロジェクトの第一弾として”ボイスコマンド”の開発に取り組み、そこで得た知見をまとめました。

---

### ボイスコマンドって？

ボイスコマンドとは、プレイヤーの声を音声認識技術により認識し、その内容に沿ったインタラクションを発生させる機能です。

考えられる活用事例としては、以下のようなものを考えています。

- 例. プレイヤーが呪文（特定の単語）を唱えた際に、VR空間（ゲーム空間）内で対応する魔法を発生させる  
- 例. プレイヤーが合言葉 (例.「ひらけゴマ」) を伝えると隠し扉が開く  

---

### 音声認識技術って前からあったと思うけど、どうして今更？

音声認識技術自体は既に色々なところで実用化されており、かつ、今までもオープンソースで利用できるものはあったものの、認識精度が十分でなかったり使い勝手が悪かったりで、なかなか実用に耐えうるものが幅広く普及するまでには至っておりませんでした。

ところが2017年にGoogleによりトランスフォーマーモデルが提唱されてから、これを取り入れることでパフォーマンスが飛躍的に向上することが分かり、優れた音声認識技術がオープンソースで提供されるようになってきました。

その中でも、[OpenAI 社が公開している Whisper モデル](https://openai.com/research/whisper)は優れた性能を有しており、人間の認識精度と同等までになっております。（[こちら](https://huggingface.co/spaces/openai/whisper)のページでWhisperを自分の声で試してみることができますので気になる方は是非！）

---

### 実際に試してみた

左の列が音声入力内容、中央と右の列は二つの条件（後述）での認識結果です。

![音声認識実験結果](/images/a9baf891ecad44/whisper_test_table.png)

初期条件では「ジャンプ」や「東京」のような、日常でよく使われる’汎用語’の認識には成功している一方で、「ambr」や「ミュウツー」といった’専門用語’の認識は上手くいっていないようです。

これは、単語の認識成功率は、その単語の学習データ内での出現頻度の高さに依存するためだと考えられます。Whisperモデルは約一万時間の日本語音声データをトレーニングに用いていて、その中で「ジャンプ」や「東京」といった単語の出現頻度が比較的高かったことが推測されます。

---

### 認識精度を上げるにはどうしたらいいの？

- **プロンプトの活用**  
    - WhisperではChatGPTと同じように、認識させる音声データと共にプロンプトを与えることで、認識結果に影響を与えることができるようになっています。  
    - 実際にプロンプトを使ってみると、認識精度が飛躍的に向上することを確認できました。  
      使用プロンプト:  
      日本?ポケモン?ミュウツー?東京?ロックマン?アンバー?ジャンプ?VR?花札?ユニティ?ひらけごま
      ※”?”記号は、単語同士を区切るために挿入しております

- **Fine Tuning**  
    - 既に学習済みの認識モデルをベースとして、更に特定の専門領域のデータを学習させることでそれらの認識率を向上させることをFine Tuningといいます。プロンプトによる方法よりは少し複雑な方法にはなりますが、既にいくつか活用事例も報告されているため、トライする価値はありそうです。

---

### どの音声認識モデルを使えばいいの？

使用するモデルを検討する際、[Whisper](https://openai.com/research/whisper) と [Meta 社製 MMS モデル](https://about.fb.com/news/2023/05/ai-massively-multilingual-speech-technology/)を比較しました。[Meta社の論文](https://research.facebook.com/publications/scaling-speech-technology-to-1000-languages/)では、日本語を含む大半の言語で認識精度（WER）が優れるという報告があります。

結果は、上記に反して、少なくとも日本語の認識精度においてはWhisperの方がMeta MMS を圧倒している印象を受けました。

![Whisper vs Meta MMS](/images/a9baf891ecad44/compare_meta.png)

一般的に深層学習モデルの性能は、パラメータ数やトレーニングデータ量に相関すると考えられています。今回、パラメータ数はMeta MMSのほうが大きかったのですが、Whisperの認識性能が高い結果となりました。原因は、モデル構造の違い（Decoder 部において、MMS は シンプルなN-grams を採用しているのに対し、Whisper では "言語らしさ" の表現に長けるAttention Mechanism を採用している）や、日本語トレーニング時間がWhisperの方が長かったからである可能性などが考えられます。Meta社の言語別トレーニング量は見つけることができませんでしたが、トレーニング総量と対応言語数から上記が推測されます。

![WhisperとMMSの比較](/images/a9baf891ecad44/compare_meta_statistics.png)

---

### 最後に

今回、実際に音声認識モデルを試してみて、改めてその性能の高さに驚かされました。開発時、初めてのテストの際、僕の「ファイア」という声が認識されて、炎が発生したときはとても感動したことを覚えております（画面の向こうでではありますが…）。このようなモデルがオープンソースに公開されていて、誰でも簡単に使うことができるのは素晴らしいことだと思います。これからも日々進化するAI技術を取り入れながら、ambr も更に進化していければと思いますのでぜひ期待していてください！

![初めてのテストの様子](/images/a9baf891ecad44/voicecomand.gif)
